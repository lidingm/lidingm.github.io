<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正 | 碎碎念的博客</title><meta name="author" content="Dingming Li"><meta name="copyright" content="Dingming Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正资料 论文：[2502.12853] S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning 代码：NineAbyss&#x2F;S2R: This is the official implementation of the paper “">
<meta property="og:type" content="article">
<meta property="og:title" content="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正">
<meta property="og:url" content="http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/index.html">
<meta property="og:site_name" content="碎碎念的博客">
<meta property="og:description" content="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正资料 论文：[2502.12853] S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning 代码：NineAbyss&#x2F;S2R: This is the official implementation of the paper “">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png">
<meta property="article:published_time" content="2025-03-06T09:24:35.000Z">
<meta property="article:modified_time" content="2025-03-08T03:40:41.411Z">
<meta property="article:author" content="Dingming Li">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正",
  "url": "http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/",
  "image": "https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png",
  "datePublished": "2025-03-06T09:24:35.000Z",
  "dateModified": "2025-03-08T03:40:41.411Z",
  "author": [
    {
      "@type": "Person",
      "name": "Dingming Li",
      "url": "https://lidingm.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/medias/head.jpg"><link rel="canonical" href="http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">碎碎念的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-06T09:24:35.000Z" title="发表于 2025-03-06 17:24:35">2025-03-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-08T03:40:41.411Z" title="更新于 2025-03-08 11:40:41">2025-03-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">论文学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a class="disqus-comment-count" href="http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="论文学习-S²R：通过强化学习教会-LLM-自我验证和自我修正"><a href="#论文学习-S²R：通过强化学习教会-LLM-自我验证和自我修正" class="headerlink" title="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正"></a>[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正</h1><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><blockquote>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.12853">[2502.12853] S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/NineAbyss/S2R">NineAbyss/S2R: This is the official implementation of the paper “S²R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning”</a></p>
</blockquote>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>​        本文提出了一种名为S²R的方法，以提高大型语言模型（LLMs）的推理能力。S²R 通过两阶段训练：首先，使用监督微调（SFT）让模型学习自我验证和自我纠正的行为；然后，通过基于强化学习（RL）的优化进一步增强这些能力。实验表明，即使在训练数据有限的情况下，S²R 也能显著提升 LLMs 在数学推理任务中的表现，优于仅使用长链思维（CoT）训练的数据集。研究还发现，自验证和自纠正能力不仅适用于数学推理，还可泛化至其他任务，如逻辑推理和代码理解。此外，论文比较了过程级和结果级强化学习的效果，并探讨了离线 RL 作为更高效的训练替代方案的可行性。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="缩放测试时间计算"><a href="#缩放测试时间计算" class="headerlink" title="缩放测试时间计算"></a>缩放测试时间计算</h3><p>​        人们越来越关注训练LLMs自己执行测试时间搜索，通常通过进行更长和更深入的思考。这些测试时间缩放工作不仅直接有益于LLM重建，而且还可以集成回训练时间，从而实现LLM重建的迭代改进 。而在这项工作中，文章提出了一个有效的框架，用于训练LLMs通过自我验证和自我纠正来进行有效的测试时间扩展。这种方法无需付出大量努力即可实现，并且S2R的性能也可以通过迭代训练得到持续提升。</p>
<h3 id="自我验证和自我纠正"><a href="#自我验证和自我纠正" class="headerlink" title="自我验证和自我纠正"></a>自我验证和自我纠正</h3><p>​        使LLMs能够执行有效的自我验证和自我纠正是实现LLMs健壮推理的有前途的解决方案 ，这些能力对于执行深度推理也至关重要。先前的研究表明，在大多数情况下，直接提示LLMs进行自我验证或自我纠正是不理想的 。而文章提出了一种通过原理性的模仿数据构建和RL训练来增强LLMs的自我验证和自我纠正能力的有效方法，并通过深入分析证明了该方法的有效性。</p>
<h3 id="RL用于LLM-Reasoning"><a href="#RL用于LLM-Reasoning" class="headerlink" title="RL用于LLM Reasoning"></a>RL用于LLM Reasoning</h3><p>​        强化学习已被证明在提高各种任务的LLM性能方面是有效的。在LLM推理中，先前的研究通常将RL用于actor-critic框架中 ，而为RL训练开发准确奖励模型的研究一直是一个长期的目标，特别是在过程级RL的奖励模型方面 。而最近研究表明，简化的奖励建模和优势估计在RL训练中也可以有效地增强LLM推理能力。在这项工作中，我们还表明，简化的优势估计和RL框架可以有效地改进LLM推理。并且文章还对过程级RL、结果级RL和离线RL进行了分析，为未来lm推理的RL工作提供了见解。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>​        S²R 主要由两个阶段组成：</p>
<p>​        PS：阶段0是指数据的构建。</p>
<ul>
<li><strong>行为初始化</strong>：使用监督微调（SFT）在 carefully curated 数据上训练模型，使其掌握初步的自我验证和自我纠正能力。</li>
<li><strong>强化学习优化</strong>：结合结果级和过程级强化学习，使模型在推理过程中不断优化自己的推理路径。</li>
</ul>
<p>​        S²R 采用的关键策略：</p>
<ul>
<li><strong>自我验证</strong>：模型在推理过程中检查自己的答案是否正确。</li>
<li><strong>自我纠正</strong>：模型在发现错误后，自主改正答案。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/lidingm/blog_img/img/202503062300243.png" alt="image-20250306230046048"></p>
<h3 id="自我验证和自我纠正行为的初始化"><a href="#自我验证和自我纠正行为的初始化" class="headerlink" title="自我验证和自我纠正行为的初始化"></a>自我验证和自我纠正行为的初始化</h3><h4 id="自我验证"><a href="#自我验证" class="headerlink" title="自我验证"></a>自我验证</h4><ul>
<li><strong>问题求解型验证</strong>：通过重新求解问题来验证答案。“问题解决式”验证直观上并非所期望的理想验证行为。</li>
<li><strong>确认型验证</strong>：通过替代方法验证答案的正确性。理想情况下，希望模型能够跳出固有思维模式，从新的视角重新审视解决方案，而非仅仅思考 从同样的问题解决视角进行验证。通过提示现有的大型语言模型“在不 重新解决问题的情况下验证答案的正确性”来 构建“确认式”验证，并使用大型语言模型作为裁判来过滤掉无效的验证。</li>
</ul>
<h4 id="自我纠正"><a href="#自我纠正" class="headerlink" title="自我纠正"></a>自我纠正</h4><p>​        LLMs通常无法通过SFT学习有效的自我纠正行为，但自我纠正的有效性可以通过强化学习来增强。只在这个阶段初始化自我纠正行为，将进一步增强自我纠正能力的工作留给RL阶段。</p>
<h4 id="构建动态试错轨迹"><a href="#构建动态试错轨迹" class="headerlink" title="构建动态试错轨迹"></a>构建动态试错轨迹</h4><ul>
<li>为了确保轨迹的多样性，构建各种长度的轨迹。</li>
<li>为了确保LLMs学会验证和纠正自己的错误，我们通过从LLMs的响应中进行抽样和筛选来构建每个轨迹中的失败试验。</li>
<li>合理的测试时间缩放方法将合理的工作分配给不同级别的问题，具体来说，在达到正确答案之前，更困难的问题需要更多的试错迭代次数。因此根据每个基础模型采样响应的准确性来确定每条轨迹的长度。</li>
</ul>
<h4 id="监督微调以初始化思维行为"><a href="#监督微调以初始化思维行为" class="headerlink" title="监督微调以初始化思维行为"></a>监督微调以初始化思维行为</h4><p><img src="https://cdn.jsdelivr.net/gh/lidingm/blog_img/img/202503062351013.png" alt="image-20250306235106945"></p>
<p>在训练过程中，通过使用掩码来优化所有验证的概率，而仅对最后一个正确 的解 $s_N$进行优化。</p>
<h3 id="通过强化学习提升思维能力"><a href="#通过强化学习提升思维能力" class="headerlink" title="通过强化学习提升思维能力"></a>通过强化学习提升思维能力</h3><h4 id="结果级强化学习RLOO"><a href="#结果级强化学习RLOO" class="headerlink" title="结果级强化学习RLOO"></a>结果级强化学习RLOO</h4><ul>
<li>目标是提升最终答案的正确率。</li>
<li>适合较强的 LLMs，如 Qwen2.5-Math-7B。</li>
</ul>
<p>​        结果级强化学习（RLOO，REINFORCE Leave-One-Out）是一种基于最终结果的强化学习方法。它通过评估最终答案的正确性来优化模型的行为轨迹。具体而言：</p>
<ul>
<li><strong>奖励函数</strong>：基于最终答案的正确性，正确答案获得正奖励（+1），错误答案获得负奖励（-1）。</li>
<li><strong>优势估计</strong>：通过留一法（Leave-One-Out）估计基线，计算每个轨迹的优势值。</li>
<li><strong>优化目标</strong>：通过最小化损失函数，优化整个轨迹的生成概率，鼓励模型探索更有效的推理路径。</li>
</ul>
<p>​        结果级强化学习的优势在于允许模型在中间步骤中自由探索，最终优化最终答案的正确性。</p>
<h4 id="过程级组强化学习"><a href="#过程级组强化学习" class="headerlink" title="过程级组强化学习"></a>过程级组强化学习</h4><ul>
<li>目标是优化推理过程的每一步。</li>
<li>适用于推理能力较弱的 LLMs，如 Qwen2-7B-Instruct。</li>
</ul>
<p>​        过程级强化学习关注推理过程中的每个步骤，通过为每个动作（如“验证”或“解决”）分配奖励来优化模型的行为。具体方法如下：</p>
<ul>
<li><strong>奖励函数</strong>：根据动作类型（验证或解决）和动作的正确性分配奖励。例如，正确的解决动作获得+1奖励，错误的验证动作获得-1奖励。</li>
<li><strong>基线估计</strong>：通过组内动作的平均奖励估计基线，确保相同奖励上下文的动作具有相似的基线。</li>
<li><strong>优化目标</strong>：通过最小化每个动作的损失函数，优化模型在每个步骤的行为。</li>
</ul>
<p>​        过程级强化学习的优势在于能够直接优化中间步骤的正确性，适合推理能力较弱的模型。</p>
<h4 id="更高效的离线强化学习"><a href="#更高效的离线强化学习" class="headerlink" title="更高效的离线强化学习"></a>更高效的离线强化学习</h4><p>​        离线强化学习是一种更高效的替代方案，能够在计算资源受限的情况下提供类似的性能提升。不依赖实时采样的强化学习方法，具有更高的效率和更稳定的训练过程。本文探索了离线强化学习在S2R框架中的应用：</p>
<ul>
<li><strong>准确性分组基线</strong>：通过问题的难度（通过准确率估计）对轨迹进行分组，进一步优化基线估计。</li>
<li><strong>损失函数</strong>：结合准确性分组和位置信息，计算每个动作的优势值，并优化模型的行为。</li>
<li><strong>实施细节</strong>：通过过滤、采样和优化步骤，离线强化学习能够在大规模数据上高效训练，减少计算资源的消耗。</li>
</ul>
<p>​        离线强化学习的优势在于能够利用大规模数据进行更准确的基线估计，同时避免了在线强化学习中的实时采样开销。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>实验数据集</strong></p>
<p>​        在7个不同的数学基准上评估了所提出的方法。为确保进行全面的评估，此外，还使用了分布式GSM8K和MATH500测试集，包括具有挑战性的分配外基准，涵盖各种难度级别和数学领域，包括AIME 2024竞争问题，AMC2023考试,来自奥林匹克长凳的高级rea-soning任务 ，以及大学数学的大学级问题集。此外，还评估了在真实世界标准化测试中的表现，即高考。</p>
<p><strong>实验模型</strong></p>
<p>​        使用三种不同的基础模型加以S²R框架进行实验:</p>
<ul>
<li>Llama-3.1-8B-Instruct</li>
<li>qwen2-7B-Instruct </li>
<li>Qwen2.5-Math-7B</li>
</ul>
<p>​        其中Llama-3.1-8B-Instruct和Qwen2-7B-Instruct是在不同领域进行训练的通用模型。Qwen2.5-Math-7B是专门为数学问题解决而定制的最先进模型。</p>
<p>​        提出的方法与四类强基准模型进行比较：</p>
<ul>
<li>Frontier LLMs：包括前沿的专有模型，如GPT-4o、最新的Claude以及OpenAI的o1-preview和o1-mini。</li>
<li>顶级开源推理模型：涵盖以强大的推理能力而闻名的最先进的开源模型，包括Mathstral-7B-v0.1，NuminaMath-72B，LLaMA3.1-70B-Instruct和Qwen2.5-Math-72B-Instruct。</li>
<li>基于Qwen2.5-Math-7B的增强模型： 鉴于最近Qwen2.5-Math-7B作为基础模型的流行，我们根据Qwen2.5-Math-7B的三个竞争基线（Eurus-2-7B-PRIME，rStar-Math-7B。和Qwen2.5-7b-implerl）评估S²R，这些模块作为我们的Qwen2.5-Math-7B-based变体的直接和强有力的基准。</li>
<li>具有不同CoT结构的SFT： 还比较了竞争性类型的CoT推理的训练，包括训练数据集中的原始CoT解决方案，和从广泛采用的开源o1-like模型 “QwQ-32B-Preview” 中提炼出来的Long-ccosso-lutions。</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>论文在多个数学推理基准数据集（如 MATH500, GSM8K）上测试了 S²R，主要结果如下：</p>
<ul>
<li>S²R 在 Qwen2.5-Math-7B 上的准确率从 51.0% 提高到 81.6%，超越了等量的长链思维（CoT）数据微调模型（80.2%）。</li>
<li>过程级强化学习（Process-level RL）更适用于初始推理能力较弱的模型，而结果级强化学习（Outcome-level RL）更能提升较强模型的推理能力。</li>
<li>S²R 泛化能力强，在跨领域任务（如逻辑推理、代码推理）中也表现良好。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/lidingm/blog_img/img/202503062333294.png" alt="image-20250306233344147"></p>
<h2 id="分析自我验证和自我纠正能力"><a href="#分析自我验证和自我纠正能力" class="headerlink" title="分析自我验证和自我纠正能力"></a>分析自我验证和自我纠正能力</h2><p>​        首先对问题解决法和确认性验证法进行比较。如图表格展示 了不同方法在 Math500 测试集上的验证结果。 其指出总体而言，问题解决验证的总体准确率通常优于确认性验证。之后在所有实验中，都使用了确认性验证来进行行为初始化。</p>
<p><img src="https://cdn.jsdelivr.net/gh/lidingm/blog_img/img/202503062343620.png" alt="image-20250306234320521"></p>
<p>​        之后研究强化学习训练对模型自我验证和自我修正能力的影响。如图中，我们展示了行为初始化模型 （SFT）以及从 Qwen2.5-Math-7B 获得的不同强化学习模型的结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/lidingm/blog_img/img/202503062344645.png" alt="image-20250306234436566"></p>
<ul>
<li>两种强化学习方法均能有效提高自我验证的准确 性。过程级强化学习在准确性方面表现出更大 的提升，而结果级强化学习则持续提高了错误 召回率和正确精度。这可能是因为过程级监督不加区分地促进了中间步骤的验证准确性，而 结果级监督允许策略模型在中间步骤自由探索， 仅提升最终答案的准确性，从而主要增强了错误召回率和正确精度。</li>
<li>两种强化学习方法都能成功提升模型的自我纠正能力。值得注意的是， 经过强化学习训练后，模型纠正错误答案的能 力显著提高，模型错误修改正确答案的比率也 显著降低。这一比较表明，S2R 能够显著增强 模型自我纠正能力的有效性。</li>
</ul>
<p>​        之后为了进一步说明 S2R 训练的效果，如图展示了  SFT 和 SFT+RL 模型在不同难度水平下的答案准确率和平均试验次数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/lidingm/blog_img/img/202503062347343.png" alt="image-20250306234715217"></p>
<ul>
<li>通过在推理过程中学习自我验证和自我纠正，模型学会了动态分配测试时的努力。对于较容易的问题，模型可以用较少的试验次数得出有把握的 答案，而对于较难的问题，则需要更多的试验次 数才能得出有把握的答案。</li>
<li>强化学习进一步改善了测试时的努力分配，特别是对于能力较弱的模型。</li>
<li>经 过强化学习训练后，较难问题的答案准确率显著提高，这表明自我验证和自我纠正范式在增强模型推理能力方面是有效的。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​        S²R 提供了一种高效的 LLMs 训练范式，使模型能够在推理过程中自主检查和优化自己的答案，不仅提升了数学推理能力，还能泛化到其他任务。通过在 多个数学推理基准数据集上的实验，S²R 展现出显著的推理能力提升，同时在逻辑推理、代码推理等跨领域任务中也表现优异。其创新性在于结合监督微调（SFT）和强化学习（RL）优化，使得 LLM 能够在有限的资源下高效学习自我验证和自我纠正策略，进而提升推理的准确性和稳定性。未来，S²R 可进一步扩展至更多复杂推理任务，如科学研究、医学诊断等，推动 LLM 在更广泛场景中的应用。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://lidingm.github.io/">Dingming Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/">http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">碎碎念的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/RL/">RL</a></div><div class="post-share"><div class="social-share" data-image="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/03/05/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSpatialRGPT%EF%BC%9A%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E8%90%BD%E5%9C%B0%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86/" title="[论文学习]SpatialRGPT：视觉语言模型中落地空间推理"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">[论文学习]SpatialRGPT：视觉语言模型中落地空间推理</div></div><div class="info-2"><div class="info-item-1">[论文学习]SpatialRGPT：视觉语言模型中落地空间推理资料 主页：SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models 代码：AnjieCheng/SpatialRGPT: [NeurIPS’24] This repository is the implementation of “SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models” 论文：[2406.01584] SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models  简介​       ...</div></div></div></a><a class="pagination-related" href="/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSCoRe%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%A9LLM%E5%AD%A6%E4%BC%9A%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3/" title="[论文学习]SCoRe：通过强化学习让LLM学会自我纠正"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">[论文学习]SCoRe：通过强化学习让LLM学会自我纠正</div></div><div class="info-2"><div class="info-item-1">[论文学习]SCoRe：通过强化学习让LLM学会自我纠正资料 论文：[2409.12917] Training Language Models to Self-Correct via Reinforcement Learning  简介​        本文提出了一种名为 SCoRe 的多轮在线强化学习方法，以提升大型语言模型（LLMs）的自我纠正能力，避免依赖额外的监督或更强的模型。研究表明，传统的监督微调（SFT）方法在训练自我纠正时常遇到分布不匹配或行为坍缩的问题，使得模型无法有效改正自身错误。SCoRe 通过在模型自身生成的错误更正轨迹上进行训练，并结合适当的正则化，成功避免这些问题。该方法在 MATH 和 HumanEval 任务上显著提升了 Gemini 1.0 Pro 和 1.5 Flash 模型的自我纠正性能，分别提高了 15.6% 和 9.1%。论文还通过实验分析了传统 SFT 失败的原因，并证明强化学习在训练自我纠正能力方面的必要性。 ​       ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSCoRe%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%A9LLM%E5%AD%A6%E4%BC%9A%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3/" title="[论文学习]SCoRe：通过强化学习让LLM学会自我纠正"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">[论文学习]SCoRe：通过强化学习让LLM学会自我纠正</div></div><div class="info-2"><div class="info-item-1">[论文学习]SCoRe：通过强化学习让LLM学会自我纠正资料 论文：[2409.12917] Training Language Models to Self-Correct via Reinforcement Learning  简介​        本文提出了一种名为 SCoRe 的多轮在线强化学习方法，以提升大型语言模型（LLMs）的自我纠正能力，避免依赖额外的监督或更强的模型。研究表明，传统的监督微调（SFT）方法在训练自我纠正时常遇到分布不匹配或行为坍缩的问题，使得模型无法有效改正自身错误。SCoRe 通过在模型自身生成的错误更正轨迹上进行训练，并结合适当的正则化，成功避免这些问题。该方法在 MATH 和 HumanEval 任务上显著提升了 Gemini 1.0 Pro 和 1.5 Flash 模型的自我纠正性能，分别提高了 15.6% 和 9.1%。论文还通过实验分析了传统 SFT 失败的原因，并证明强化学习在训练自我纠正能力方面的必要性。 ​       ...</div></div></div></a><a class="pagination-related" href="/2024/11/08/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DATTEMPT-%E9%80%9A%E8%BF%87%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B7%B7%E5%90%88%E8%BD%AF%E6%8F%90%E7%A4%BA%E8%BF%9B%E8%A1%8C%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E8%B0%83%E6%95%B4/" title="[论文学习]ATTEMPT:通过注意力混合软提示进行参数高效的多任务调整"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-08</div><div class="info-item-2">[论文学习]ATTEMPT:通过注意力混合软提示进行参数高效的多任务调整</div></div><div class="info-2"><div class="info-item-1">论文学习：ATTEMPT:通过注意力混合软提示进行参数高效的多任务调整资料 论文：ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts 代码：AkariAsai/ATTEMPT: This is the oficial repository for “Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft...</div></div></div></a><a class="pagination-related" href="/2025/03/05/%5B%E6%A6%82%E8%BF%B0%E5%AD%A6%E4%B9%A0%5D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%9A%E5%BC%88%E4%B8%AD%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/" title="[概述学习]语言模型在多智能体博弈中的推理能力"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-05</div><div class="info-item-2">[概述学习]语言模型在多智能体博弈中的推理能力</div></div><div class="info-2"><div class="info-item-1">[概述学习]语言模型在多智能体博弈中的推理能力 转载自语言模型在多智能体博弈中的推理能力-CSDN博客  关键词 语言模型 多智能体博弈 推理能力 深度学习 策略学习  摘要​        本文探讨了语言模型在多智能体博弈中的推理能力。通过分析语言模型的原理和多智能体博弈的特点，我们探讨了语言模型在博弈中的具体应用场景和推理过程。文章首先介绍了语言模型和多智能体博弈的基本概念，然后详细阐述了语言模型在多智能体博弈中的推理算法，最后通过具体案例展示了语言模型在博弈中的应用效果。 第一部分：背景介绍1.1 问题背景​       ...</div></div></div></a><a class="pagination-related" href="/2025/03/04/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DLLMs%E7%9A%84%E5%BF%83%E7%81%B5%E4%B9%8B%E7%9C%BC%EF%BC%9A%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%80%9D%E7%BB%B4%E8%AF%B1%E5%AF%BC%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/" title="[论文学习]LLMs的心灵之眼：大型语言模型中思维诱导空间推理的可视化"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-04</div><div class="info-item-2">[论文学习]LLMs的心灵之眼：大型语言模型中思维诱导空间推理的可视化</div></div><div class="info-2"><div class="info-item-1">[论文学习]LLMs的心灵之眼：大型语言模型中思维诱导空间推理的可视化资料 主页：Mind’s eye of LLMs Vot 论文：[2404.03622] Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models 代码：microsoft/visualization-of-thought: [NeurIPS 2024]Repos for “Visualization-of-Thought” dataset, construction code and evaluation.  简介2024年的NeurIPS会议 ​        论文提出了一种名为“Visualization-of-Thought（VoT）”的提示方法，旨在通过可视化大型语言模型（LLMs）的推理过程来激发其空间推理能力。人类在进行空间推理时，能够通过“心灵之眼”（Mind’s...</div></div></div></a><a class="pagination-related" href="/2025/03/05/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5D%E8%AF%AD%E8%A8%80%E7%AD%96%E7%95%A5%E7%9A%84%E5%8D%9A%E5%BC%88%E8%AE%BA%E6%96%B0%E5%A2%83%E7%95%8C%EF%BC%9A%E4%BB%8E%E5%AF%B9%E8%AF%9D%E5%88%B0%E5%B9%B3%E8%A1%A1%E2%80%94%E2%80%94%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%9A%E5%BC%88%E8%A7%A3%E6%9E%84%E4%B8%8E%E5%89%8D%E7%9E%BB/" title="[论文学习]语言策略的博弈论新境界：从对话到平衡——大语言模型的博弈解构与前瞻"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-05</div><div class="info-item-2">[论文学习]语言策略的博弈论新境界：从对话到平衡——大语言模型的博弈解构与前瞻</div></div><div class="info-2"><div class="info-item-1">[论文学习]语言策略的博弈论新境界：从对话到平衡——大语言模型的博弈解构与前瞻 转载自语言策略的博弈论新境界：从对话到平衡——大语言模型的博弈解构与前瞻-CSDN博客  资料 论文：[2402.01704] Steering Language Models with Game-Theoretic Solvers 代码：open_spiel/open_spiel/python/games/chat_game.py at master · google-deepmind/open_spiel  ​        在人工智能日新月异的发展中，我们常见到一台台大语言模型（LLM）在聊天、问答与创作中大放异彩。然而，在这些机智回答的背后，却隐藏着一个尚未充分挖掘的秘密：对话不仅仅是文字的堆砌，更是一场复杂的多主体战略博弈。最新研究《States as Strings as Strategies: Steering Language Models with Game-Theoretic...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/medias/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Dingming Li</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lidingm"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0-S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A-LLM-%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3"><span class="toc-number">1.</span> <span class="toc-text">[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%96%99"><span class="toc-number">1.1.</span> <span class="toc-text">资料</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%A9%E6%94%BE%E6%B5%8B%E8%AF%95%E6%97%B6%E9%97%B4%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.1.</span> <span class="toc-text">缩放测试时间计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3"><span class="toc-number">1.3.2.</span> <span class="toc-text">自我验证和自我纠正</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RL%E7%94%A8%E4%BA%8ELLM-Reasoning"><span class="toc-number">1.3.3.</span> <span class="toc-text">RL用于LLM Reasoning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3%E8%A1%8C%E4%B8%BA%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.4.1.</span> <span class="toc-text">自我验证和自我纠正行为的初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">自我验证</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">自我纠正</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%8A%A8%E6%80%81%E8%AF%95%E9%94%99%E8%BD%A8%E8%BF%B9"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">构建动态试错轨迹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%E4%BB%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E6%80%9D%E7%BB%B4%E8%A1%8C%E4%B8%BA"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">监督微调以初始化思维行为</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87%E6%80%9D%E7%BB%B4%E8%83%BD%E5%8A%9B"><span class="toc-number">1.4.2.</span> <span class="toc-text">通过强化学习提升思维能力</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E7%BA%A7%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0RLOO"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">结果级强化学习RLOO</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B%E7%BA%A7%E7%BB%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">过程级组强化学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9B%B4%E9%AB%98%E6%95%88%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">更高效的离线强化学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.5.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.5.1.</span> <span class="toc-text">实验设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.5.2.</span> <span class="toc-text">实验结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3%E8%83%BD%E5%8A%9B"><span class="toc-number">1.6.</span> <span class="toc-text">分析自我验证和自我纠正能力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.7.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DMM-EGO%EF%BC%9A%E8%BF%88%E5%90%91%E6%9E%84%E5%BB%BA%E4%BB%A5%E8%87%AA%E6%88%91%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81LLMS/" title="[论文学习]MM-EGO：迈向构建以自我为中心的多模态LLMS"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="[论文学习]MM-EGO：迈向构建以自我为中心的多模态LLMS"/></a><div class="content"><a class="title" href="/2025/03/07/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DMM-EGO%EF%BC%9A%E8%BF%88%E5%90%91%E6%9E%84%E5%BB%BA%E4%BB%A5%E8%87%AA%E6%88%91%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81LLMS/" title="[论文学习]MM-EGO：迈向构建以自我为中心的多模态LLMS">[论文学习]MM-EGO：迈向构建以自我为中心的多模态LLMS</a><time datetime="2025-03-07T10:46:14.000Z" title="发表于 2025-03-07 18:46:14">2025-03-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSCoRe%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%A9LLM%E5%AD%A6%E4%BC%9A%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3/" title="[论文学习]SCoRe：通过强化学习让LLM学会自我纠正"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="[论文学习]SCoRe：通过强化学习让LLM学会自我纠正"/></a><div class="content"><a class="title" href="/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSCoRe%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%A9LLM%E5%AD%A6%E4%BC%9A%E8%87%AA%E6%88%91%E7%BA%A0%E6%AD%A3/" title="[论文学习]SCoRe：通过强化学习让LLM学会自我纠正">[论文学习]SCoRe：通过强化学习让LLM学会自我纠正</a><time datetime="2025-03-06T11:14:51.000Z" title="发表于 2025-03-06 19:14:51">2025-03-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DS%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/" title="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正"/></a><div class="content"><a class="title" href="/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DS%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/" title="[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正">[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正</a><time datetime="2025-03-06T09:24:35.000Z" title="发表于 2025-03-06 17:24:35">2025-03-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/05/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSpatialRGPT%EF%BC%9A%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E8%90%BD%E5%9C%B0%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86/" title="[论文学习]SpatialRGPT：视觉语言模型中落地空间推理"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="[论文学习]SpatialRGPT：视觉语言模型中落地空间推理"/></a><div class="content"><a class="title" href="/2025/03/05/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DSpatialRGPT%EF%BC%9A%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E8%90%BD%E5%9C%B0%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86/" title="[论文学习]SpatialRGPT：视觉语言模型中落地空间推理">[论文学习]SpatialRGPT：视觉语言模型中落地空间推理</a><time datetime="2025-03-05T14:23:16.000Z" title="发表于 2025-03-05 22:23:16">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/05/%5B%E6%A6%82%E8%BF%B0%E5%AD%A6%E4%B9%A0%5D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%9A%E5%BC%88%E4%B8%AD%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/" title="[概述学习]语言模型在多智能体博弈中的推理能力"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="[概述学习]语言模型在多智能体博弈中的推理能力"/></a><div class="content"><a class="title" href="/2025/03/05/%5B%E6%A6%82%E8%BF%B0%E5%AD%A6%E4%B9%A0%5D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%9A%E5%BC%88%E4%B8%AD%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/" title="[概述学习]语言模型在多智能体博弈中的推理能力">[概述学习]语言模型在多智能体博弈中的推理能力</a><time datetime="2025-03-05T08:13:45.000Z" title="发表于 2025-03-05 16:13:45">2025-03-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Dingming Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'

  const disqusReset = conf => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: conf
    })
  }

  const loadDisqus = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyDisqus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    window.disqus_identifier = isShuoshuo ? path : '/2025/03/06/%5B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%5DS%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/'
    window.disqus_url = isShuoshuo ? location.origin + path : 'http://example.com/2025/03/06/[%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0]S%C2%B2R%EF%BC%9A%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E4%BC%9A%20LLM%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%E5%92%8C%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3/'

    const disqus_config = function () {
      this.page.url = disqus_url
      this.page.identifier = disqus_identifier
      this.page.title = '[论文学习]S²R：通过强化学习教会 LLM 自我验证和自我修正'
    }

    if (window.DISQUS) disqusReset(disqus_config)
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }

    btf.addGlobalFn('themeChange', () => disqusReset(disqus_config), 'disqus')
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if (isShuoshuo) {
    'Disqus' === 'Disqus'
      ? window.shuoshuoComment = { loadComment: loadDisqus }
      : window.loadOtherComment = loadDisqus
    return
  }

  if ('Disqus' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.pageType === 'post' && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>